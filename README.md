# LatticeBench â€” A Discrete Gauge Theory Playground (PoC)

**English / æ—¥æœ¬èª**

---

## ğŸ§© Overview | æ¦‚è¦

**English**  
**LatticeBench** is a **minimal proof-of-concept (PoC)** platform for experimenting with
**discrete space-time structures** using a **Physics-Informed Neural Network (PINN) on a 2D SU(2) lattice**.

In conventional lattice QCD, exploring new physical structures requires
heavy modification of large-scale HPC frameworks (e.g., QUDA, Chroma, openQCD)
and massive computational resources. This makes **testing new theoretical structures
or principles very difficult** for individuals.

LatticeBench aims to be a **lightweight numerical playground** where one can
**embed physical constraints (unitarity, gauge symmetry) into the network structure**,
and **learn to reproduce gauge-invariant observables (plaquette, Wilson loops, Creutz ratio)**.

Unlike typical PINNs (which approximate **continuous PDEs**),
LatticeBench attempts to explore **discrete space-time structures themselves** â€”
a direction rarely explored so far.

This is **not a validated physical model**, but intended as an **idea prototype** for those
interested in **structure-informed neural approaches to lattice gauge theory and beyond**.

---

**æ—¥æœ¬èª**  
**LatticeBench** ã¯ã€**æ™‚ç©ºãŒæœ¬è³ªçš„ã«é›¢æ•£çš„ã‹ã‚‚ã—ã‚Œãªã„**ã¨ã„ã†ä»®å®šã®ã‚‚ã¨ã«ã€
**2æ¬¡å…ƒ SU(2) æ ¼å­ä¸Šã§ã®ç‰©ç†ã‚¤ãƒ³ãƒ•ã‚©ãƒ¼ãƒ ãƒ‰ NNï¼ˆPINNï¼‰** ã‚’æ§‹ç¯‰ã™ã‚‹
**æœ€å°é™ã®æ¦‚å¿µå®Ÿè¨¼ï¼ˆPoCï¼‰** å®Ÿè£…ã§ã™ã€‚

å¾“æ¥ã®æ ¼å­ QCD ã§ã¯ã€æ–°ã—ã„ç‰©ç†æ§‹é€ ã‚’æ¤œè¨¼ã™ã‚‹ã«ã¯
QUDAãƒ»Chromaãƒ»openQCD ãªã©ã® **å¤§è¦æ¨¡ HPC ã‚³ãƒ¼ãƒ‰** ã‚’æ”¹é€ ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã€
è†¨å¤§ãªè¨ˆç®—è³‡æºã¨å°‚é–€çŸ¥è­˜ã‚’è¦ã™ã‚‹ãŸã‚ã€**å€‹äººãŒç†è«–æ§‹é€ ã‚’æŸ”è»Ÿã«è©¦ã™ã“ã¨ã¯æ¥µã‚ã¦å›°é›£**ã§ã—ãŸã€‚

LatticeBench ã¯ã€**ç‰©ç†çš„åˆ¶ç´„ï¼ˆãƒ¦ãƒ‹ã‚¿ãƒªãƒ†ã‚£ãƒ»ã‚²ãƒ¼ã‚¸å¯¾ç§°æ€§ï¼‰ã‚’ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹é€ ã«çµ„ã¿è¾¼ã¿**ã€
**ãƒ—ãƒ©ãƒ¼ã‚¯ã‚¨ãƒƒãƒˆã‚„ Wilson ãƒ«ãƒ¼ãƒ—ã€ã‚¯ãƒ«ãƒ¼ãƒ„æ¯”ã¨ã„ã£ãŸã‚²ãƒ¼ã‚¸ä¸å¤‰é‡** ã‚’å†ç¾ã™ã‚‹ã‚ˆã†ã«å­¦ç¿’ã•ã›ã‚‹ã“ã¨ã§ã€
**æ–°ã—ã„ç†è«–æ§‹é€ ã‚’å°è¦æ¨¡ã«æ•°å€¤æ¤œè¨¼ã§ãã‚‹å®Ÿé¨“çš„ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ** ã‚’ç›®æŒ‡ã—ã¾ã™ã€‚

ã“ã‚Œã¯ã€**é€£ç¶šå ´æ–¹ç¨‹å¼ã®è¿‘ä¼¼å™¨ã¨ã—ã¦ä½¿ã‚ã‚Œã¦ããŸ PINN ã‚’**
**ã‚ãˆã¦é›¢æ•£çš„ãªæ™‚ç©ºæ§‹é€ ãã®ã‚‚ã®ã®æ¢ç´¢ã«å¿œç”¨ã™ã‚‹** ã¨ã„ã†ã€
ã“ã‚Œã¾ã§ã«ã»ã¨ã‚“ã©ä¾‹ã®ãªã„è©¦ã¿ã§ã™ã€‚

**ç‰©ç†çš„å¦¥å½“æ€§ã¯æœªæ¤œè¨¼** ã§ã‚ã‚Šã€
**æ§‹é€ åˆ¶ç´„ä»˜ã NN ã‚’æ ¼å­ã‚²ãƒ¼ã‚¸ç†è«–ã«å¿œç”¨ã™ã‚‹ç€æƒ³ã‚’æä¾›ã™ã‚‹** ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ã„ã¾ã™ã€‚

---

## ğŸš§ Limitations & Future Work | åˆ¶ç´„ã¨ä»Šå¾Œã®å±•æœ›

- **Scope (PoC):** This repository favors **experimentability over validity**. No claim of physical correctness.
- **Small lattice / SU(2):** Default experiments use small periodic lattices with the **SU(2)** gauge group.
- **Model reuse & inference-only workflows:** *Not assumed* in the current PoC.  
  However, as lattice **dimensionality/complexity increases** (e.g., SU(3) or 3D/4D), saving and **reusing trained models** (for inference or as **warm starts / fine-tuning** bases) may become meaningful.

â€”

- **ã‚¹ã‚³ãƒ¼ãƒ—ï¼ˆPoCï¼‰:** æœ¬ãƒªãƒã‚¸ãƒˆãƒªã¯ **ç‰©ç†çš„ãªå³å¯†æ€§ã‚ˆã‚Šå®Ÿé¨“å®¹æ˜“æ€§ã‚’é‡è¦–** ã—ã¦ã„ã¾ã™ã€‚
- **å°è¦æ¨¡æ ¼å­ / SU(2):** æ—¢å®šã§ã¯ **å°ã•ãªå‘¨æœŸæ ¼å­**ãƒ»**SU(2)** ã‚’å‰æã«ã—ã¦ã„ã¾ã™ã€‚
- **ãƒ¢ãƒ‡ãƒ«å†åˆ©ç”¨ãƒ»æ¨è«–å°‚ç”¨:** ç¾æ®µéšã§ã¯ **å‰æã¨ã—ã¦ã„ã¾ã›ã‚“**ã€‚  
  ãŸã ã—ã€**æ¬¡å…ƒ/è¤‡é›‘ã•ã®æ‹¡å¤§**ï¼ˆä¾‹ï¼šSU(3) ã‚„ 3D/4Dï¼‰ã§ã¯ã€**å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ãƒ»å†åˆ©ç”¨**ï¼ˆæ¨è«–ã‚„ **ã‚¦ã‚©ãƒ¼ãƒ ã‚¹ã‚¿ãƒ¼ãƒˆ / å¾®èª¿æ•´**ï¼‰ã®æœ‰åŠ¹æ€§ãŒé«˜ã¾ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

> Looking ahead, if we ever succeed in building a PINN structure that reproduces target observables with **sufficiently small loss**, such solutions may offer **hints toward better theoretical formulations**â€”even if they are still numeric surrogates rather than validated physical models.  
> å°†æ¥ã«å‘ã‘ã¦è¨€ãˆã°ã€**æå¤±ãŒååˆ†ã«å°ã•ã„ PINN æ§‹é€ **ã‚’ç¢ºç«‹ã§ããŸå ´åˆã€å³å¯†ãªç‰©ç†ãƒ¢ãƒ‡ãƒ«ã«ã¯è‡³ã‚‰ãªãã¦ã‚‚ã€**ç†è«–çš„æ çµ„ã¿ã¸ã®ãƒ’ãƒ³ãƒˆ**ã‚’ä¸ãˆã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

---

## ğŸ” Model Analysis & Interpretability | ãƒ¢ãƒ‡ãƒ«è§£æã¨å¯è§£é‡ˆæ€§

**English**  
To keep claims scientifically cautious while extracting insight from trained models, we propose the following **analysis protocol**. Each item is phrased to avoid over-claiming and focuses on verifiable properties on a 2D SU(2) lattice.

1. **Gauge-consistency checks (invariance/equivariance)**  
   - Verify that predictions of gauge-invariant quantities (plaquette/Wilson loops) are **unchanged under random local SU(2) gauge transforms** of inputs.  
   - Where alignment is used, report metrics both **before** and **after** gauge alignment to quantify reliance on alignment.

2. **Layer-wise constraint diagnostics**  
   - Monitor unitarity deviation per layer/output: `||U^â€  U - I||_F`.  
   - Track how much each loss term (plaquette, Wilson, Creutz, unitarity, smoothness) **contributes to total loss** at convergence.

3. **Sensitivity analysis (saliency/Jacobians)**  
   - Compute `âˆ‚L/âˆ‚U_{x,Î¼}` and aggregate by local motifs (links, plaquettes, rectangles) to see **which structures drive the loss**.  
   - Compare sensitivities across loss types (MSE vs Huber) to identify **robust vs brittle** contributions.

4. **Probing internal representations**  
   - Train **linear probes** from hidden features to reconstruct gauge-invariant observables (plaquette trace, Wilson loops not used by the loss).  
   - If simple probes succeed on **hold-out loop shapes/sizes** (excluded from loss), it suggests the network has learned **useful inductive structure** beyond targets.

5. **Ablation & constraint toggling**  
   - Retrain while removing or scaling individual loss terms; measure shifts in the **empirical Pareto front** (`GA-RMSE` vs `|Î”TrP|`).  
   - This isolates **which constraints are necessary/sufficient** for particular behaviors.

6. **Cross-seed stability & similarity**  
   - Train multiple seeds; report variance of metrics and **representation similarity** (e.g., CKA) to assess whether learned structure is **seed-robust** or accidental.

7. **Generalization beyond training targets (validation-only observables)**  
   - Evaluate **observables intentionally excluded from the loss** (e.g., larger Wilson loops).  
   - These serve as **validation-only metrics**: if reproduced well, they indicate the model has captured structure beyond direct fitting.

8. **Baseline comparisons (sanity checks)**  
   - Compare against simple baselines (random SU(2) fields, smoothed/random-phase fields) and, where available, **small-lattice Monte Carlo** or strong-coupling estimates for the same observables.  
   - Report effect sizes, not just p-values, to avoid overstating small differences on tiny lattices.

**Caveats.** Low loss on a tiny lattice **does not imply** a correct continuum theory or scaling behavior. Results may be **non-identifiable** up to gauge and other symmetries; always report the evaluation protocol (gauge treatment, validation-only observables, seeds).

---

**æ—¥æœ¬èª**  
ä¸»å¼µã‚’æ…é‡ã«ä¿ã¡ã¤ã¤å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ç¤ºå”†ã‚’å¾—ã‚‹ãŸã‚ã€ä»¥ä¸‹ã® **è§£æãƒ—ãƒ­ãƒˆã‚³ãƒ«**ã‚’ææ¡ˆã—ã¾ã™ã€‚ã„ãšã‚Œã‚‚ 2D SU(2) ã®å°æ ¼å­ã§æ¤œè¨¼å¯èƒ½ãªç¯„å›²ã«ç•™ã‚ã¦ã„ã¾ã™ã€‚

1. **ã‚²ãƒ¼ã‚¸æ•´åˆæ€§ï¼ˆä¸å¤‰æ€§/ç­‰å¤‰æ€§ï¼‰ãƒã‚§ãƒƒã‚¯**  
   - å…¥åŠ›ã« **å±€æ‰€ SU(2) ã‚²ãƒ¼ã‚¸å¤‰æ›**ã‚’ãƒ©ãƒ³ãƒ€ãƒ é©ç”¨ã—ã¦ã‚‚ã€ãƒ—ãƒ©ãƒ¼ã‚¯ã‚¨ãƒƒãƒˆã‚„ Wilson ãƒ«ãƒ¼ãƒ—ãªã©ã® **ã‚²ãƒ¼ã‚¸ä¸å¤‰é‡ã®äºˆæ¸¬ãŒä¸å¤‰**ã§ã‚ã‚‹ã‹ç¢ºèªã€‚  
   - ã‚²ãƒ¼ã‚¸ã‚¢ãƒ©ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆã‚’ç”¨ã„ã‚‹å ´åˆã¯ã€**å‰å¾Œã®æŒ‡æ¨™**ã‚’ä½µè¨˜ã—ã¦ã€æ•´åˆã®ä¾å­˜åº¦ã‚’æ˜ç¤ºã€‚

2. **å±¤åˆ¥ã®åˆ¶ç´„è¨ºæ–­**  
   - å„å±¤/å‡ºåŠ›ã§ã®ãƒ¦ãƒ‹ã‚¿ãƒªãƒ†ã‚£é€¸è„± `||U^â€  U - I||_F` ã‚’ç›£è¦–ã€‚  
   - åæŸæ™‚ç‚¹ã§å„æå¤±é …ï¼ˆãƒ—ãƒ©ãƒ¼ã‚¯ã‚¨ãƒƒãƒˆã€Wilsonã€Creutzã€ãƒ¦ãƒ‹ã‚¿ãƒªãƒ†ã‚£ã€ã‚¹ãƒ ãƒ¼ã‚¹ï¼‰ã® **å¯„ä¸ç‡**ã‚’åˆ†è§£ã€‚

3. **æ„Ÿåº¦è§£æï¼ˆã‚µãƒªã‚¨ãƒ³ã‚·ãƒ¼/ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ï¼‰**  
   - `âˆ‚L/âˆ‚U_{x,Î¼}` ã‚’è¨ˆç®—ã—ã€ãƒªãƒ³ã‚¯/ãƒ—ãƒ©ãƒ¼ã‚¯ã‚¨ãƒƒãƒˆ/é•·æ–¹å½¢ãªã©ã® **å±€æ‰€æ§‹é€ ã”ã¨ã«é›†ç´„**ã—ã¦ã€æå¤±ã‚’æ”¯é…ã™ã‚‹æ§‹é€ ã‚’ç‰¹å®šã€‚  
   - MSE ã¨ Huber ã§æ„Ÿåº¦ã® **é ‘å¥/è„†å¼±ãªå·®**ã‚’æ¯”è¼ƒã€‚

4. **å†…éƒ¨è¡¨ç¾ã¸ã®ãƒ—ãƒ­ãƒ¼ãƒ“ãƒ³ã‚°**  
   - ä¸­é–“ç‰¹å¾´ã‹ã‚‰ **ç·šå½¢ãƒ—ãƒ­ãƒ¼ãƒ–**ã§ã€å­¦ç¿’ã«ç›´æ¥ä½¿ã£ã¦ã„ãªã„ã‚²ãƒ¼ã‚¸ä¸å¤‰é‡ï¼ˆä¾‹ï¼šæœªä½¿ç”¨ã‚µã‚¤ã‚ºã® Wilson ãƒ«ãƒ¼ãƒ—ï¼‰ã‚’å†æ§‹æˆã€‚  
   - **ãƒ›ãƒ¼ãƒ«ãƒ‰ã‚¢ã‚¦ãƒˆå½¢çŠ¶/ã‚µã‚¤ã‚º**ã§å†ç¾æ€§ãŒé«˜ã‘ã‚Œã°ã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¶…ãˆã® **æ§‹é€ å­¦ç¿’**ã®ç¤ºå”†ã€‚

5. **ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆåˆ¶ç´„ã®åˆ‡æ›¿ï¼‰**  
   - å€‹åˆ¥æå¤±é …ã®å‰Šé™¤/ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã§å†å­¦ç¿’ã—ã€**çµŒé¨“çš„ãƒ‘ãƒ¬ãƒ¼ãƒˆå‰ç·š**ï¼ˆ`GA-RMSE` ã¨ `|Î”TrP|`ï¼‰ã®ç§»å‹•ã‚’è¦³å¯Ÿã€‚  
   - ã©ã®åˆ¶ç´„ãŒ **å¿…è¦/ååˆ†**ã‹ã‚’åˆ‡ã‚Šåˆ†ã‘ã€‚

6. **ã‚·ãƒ¼ãƒ‰é–“å®‰å®šæ€§ã¨è¡¨ç¾é¡ä¼¼**  
   - è¤‡æ•°ã‚·ãƒ¼ãƒ‰å­¦ç¿’ã§æŒ‡æ¨™åˆ†æ•£ã¨ **è¡¨ç¾é¡ä¼¼åº¦**ï¼ˆä¾‹ï¼šCKAï¼‰ã‚’å ±å‘Šã—ã€å­¦ç¿’æ§‹é€ ãŒå¶ç„¶ã§ã¯ãªã„ã‹ã‚’è©•ä¾¡ã€‚

7. **å­¦ç¿’ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤–ã¸ã®ä¸€èˆ¬åŒ–ï¼ˆæ¤œè¨¼å°‚ç”¨è¦³æ¸¬é‡ï¼‰**  
   - ä¸€éƒ¨ã®è¦³æ¸¬é‡ï¼ˆä¾‹ï¼šã‚ˆã‚Šå¤§ããª Wilson ãƒ«ãƒ¼ãƒ—ï¼‰ã‚’ **loss ã‹ã‚‰é™¤å¤–**ã—ã€**æ¤œè¨¼å°‚ç”¨ã®è©•ä¾¡æŒ‡æ¨™**ã¨ã—ã¦åˆ©ç”¨ã€‚  
   - è‰¯å¥½ã«å†ç¾ã§ãã‚Œã°ã€å˜ãªã‚‹å½“ã¦ã¯ã‚ã§ã¯ãªã **æ§‹é€ çš„ç†è§£ã®ç²å¾—**ã‚’ç¤ºå”†ã€‚

8. **ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¯”è¼ƒï¼ˆå¥å…¨æ€§ç¢ºèªï¼‰**  
   - ç°¡æ˜“ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆãƒ©ãƒ³ãƒ€ãƒ  SU(2) å ´ã€ä½ç›¸å¹³æ»‘åŒ–å ´ãªã©ï¼‰ã‚„ã€å¯èƒ½ãªã‚‰ **å°è¦æ¨¡æ ¼å­ã®ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­**/å¼·çµåˆè¿‘ä¼¼ã®åŒä¸€è¦³æ¸¬é‡ã¨æ¯”è¼ƒã€‚  
   - å°æ ¼å­ã‚†ãˆã®å¶ç„¶æ€§ã‚’é¿ã‘ã‚‹ãŸã‚ã€på€¤ã ã‘ã§ãªã **åŠ¹æœé‡**ã‚‚ä½µè¨˜ã€‚

**æ³¨æ„.** å°ã•ãªæ ¼å­ã§ã®ä½æå¤±ã¯ã€é€£ç¶šæ¥µé™ã‚„ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®æ­£ã—ã•ã‚’ **ä¿è¨¼ã—ã¾ã›ã‚“**ã€‚çµæœã¯ã‚²ãƒ¼ã‚¸ç­‰ã®å¯¾ç§°æ€§ã«ã‚ˆã‚Š **åŒå®šä¸èƒ½**ãªå ´åˆãŒã‚ã‚‹ãŸã‚ã€è©•ä¾¡æ‰‹é †ï¼ˆã‚²ãƒ¼ã‚¸å‡¦ç†ã€æ¤œè¨¼å°‚ç”¨è¦³æ¸¬é‡ã€ã‚·ãƒ¼ãƒ‰ï¼‰ã‚’å¿…ãšæ˜è¨˜ã—ã¦ãã ã•ã„ã€‚

---

## âš™ï¸ Features | ç‰¹å¾´
- SU(2) gauge group on **small periodic lattices** (default examples use 4Ã—4).
- **Structure-aware** approach:
  - SU(2) exponential map for strict unitarity
  - Smoothness and L2 regularization on embeddings and algebra outputs
- **Loss components:**
  - Plaquette traces (complex MSE)
  - Wilson loops (1Ã—1, 1Ã—2, 2Ã—2, 1Ã—3, 2Ã—3)
  - Creutz ratio Ï‡(2,2)
  - Unitarity / smoothness penalties
- **Evaluation metrics:**
  - Gauge-aligned link RMSE
  - Mean plaquette trace error (|Î”TrP|)

---

## ğŸ“ˆ Evaluation | è©•ä¾¡æŒ‡æ¨™
- `|Î”Tr P|`: å¹³å‡ãƒ—ãƒ©ãƒ¼ã‚¯ã‚¨ãƒƒãƒˆèª¤å·®
- `Wilson(Rx,Ry)`: æ±åŒ–ç¢ºèªï¼ˆæœªä½¿ç”¨ã‚µã‚¤ã‚ºå«ã‚€ï¼‰
- `Creutz Ï‡(2,2)`: é¢å¼µåŠ›æ¨å®š
- `Gauge-aligned RMSE`: ã‚²ãƒ¼ã‚¸å¤‰æ›è‡ªç”±åº¦ã‚’é™¤ã„ãŸãƒªãƒ³ã‚¯ RMSE

---

## ğŸ“¦ Requirements & Setup | å¿…è¦ç’°å¢ƒã¨ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

Minimal requirements (CPU execution):

```text
numpy
torch
pandas
matplotlib
optuna
```

> Note: `requirements.txt` deliberately excludes GPUâ€‘specific builds of PyTorch.  
> äº‹å‰ã«ç’°å¢ƒã«åˆã£ãŸ PyTorch ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ã‹ã‚‰ `pip install -r requirements.txt` ã‚’æ¨å¥¨ã—ã¾ã™ã€‚

### GPU / CUDA (optional but recommended) | GPU / CUDAï¼ˆæ¨å¥¨ï¼‰

Install PyTorch with CUDA support **before** installing `requirements.txt`.
Check the official PyTorch site for the correct build for your system.

**Linux / Windows (CUDA 12.1 example)**

```bash
pip install --index-url https://download.pytorch.org/whl/cu121 torch torchvision
pip install -r requirements.txt
```

**CPU only**

```bash
pip install torch torchvision   # CPU build
pip install -r requirements.txt
```

**Apple Silicon (MPS)**

```bash
pip install torch torchvision   # MPS build
pip install -r requirements.txt
```

**Quick check**

```bash
python - <<'PY'
import torch
print("cuda? ", torch.cuda.is_available())
if torch.cuda.is_available():
    print("gpu  : ", torch.cuda.get_device_name(0))
print("mps?  ", getattr(torch.backends, 'mps', None) and torch.backends.mps.is_available())
PY
```

---

## ğŸš€ Usage | ä½¿ã„æ–¹

**Train once / å˜ç™ºå­¦ç¿’**

```bash
python -m src.train \
  --epochs 700 --print_every 100 \
  --lr 5e-3 --seed 777 \
  --use_huber --huber_delta_wil 0.003 --huber_delta_cr 0.008 \
  --w_wil11 0.10 --w_wil12 0.56 --w_wil22 0.28 --w_wil13 0.20 --w_wil23 0.16 \
  --w_cr 0.55 \
  --w_plaq 0.04 \
  --w_unitary 0.06 \
  --w_phi_smooth 0.04 --w_theta_smooth 0.02 --w_phi_l2 0.002
```

Outputs are written to `runs/*.log` (if you tee logs yourself).

---

## ğŸ§ª Batch Sweep (run.bash) | ä¸€æ‹¬ã‚¹ã‚¤ãƒ¼ãƒ—

**English**  
`run.bash` launches a grid search over several loss weights, learning rates, Huber on/off, and seeds.
Each run writes a human-readable log to `runs/*.log`. It can take many hours on CPU.

**æ—¥æœ¬èª**  
`run.bash` ã¯è¤‡æ•°ã®æå¤±é‡ã¿ãƒ»å­¦ç¿’ç‡ãƒ»Huber æœ‰ç„¡ãƒ»ã‚·ãƒ¼ãƒ‰ã®ã‚°ãƒªãƒƒãƒ‰æ¢ç´¢ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
å„å®Ÿè¡Œã®ãƒ­ã‚°ã¯ `runs/*.log` ã«ä¿å­˜ã•ã‚Œã¾ã™ã€‚CPU ã§ã¯é•·æ™‚é–“ï¼ˆæ•°æ™‚é–“ã€œï¼‰ã‹ã‹ã‚Šã¾ã™ã€‚

**Run**

```bash
bash run.bash
# logs -> runs/A_*.log, runs/B_*.log
```

**Notes | æ³¨æ„**  
- Edit arrays in the header of `run.bash` to adjust the sweep.  
  ï¼ˆã‚¹ã‚¯ãƒªãƒ—ãƒˆå…ˆé ­ã®é…åˆ—ã‚’ç·¨é›†ã—ã¦æ¢ç´¢ç¯„å›²ã‚’èª¿æ•´ã§ãã¾ã™ï¼‰
- `runs/` is `.gitignore`-ed.  
  ï¼ˆ`runs/` ã¯ `.gitignore` æ¸ˆã¿ï¼‰

---

## ğŸ” Hyperparameter Search with Optuna | Optunaã«ã‚ˆã‚‹æ¢ç´¢

**English**  
We provide an **Optuna-based search script** (`src/optuna_search.py`) for flexible tuning.  
It explores the **Pareto front** between `GA-RMSE` and `|Î”TrP|`.

**æ—¥æœ¬èª**  
æŸ”è»Ÿãªãƒã‚¤ãƒ‘ãƒ©æ¢ç´¢ã®ãŸã‚ã« **Optuna ã‚¹ã‚¯ãƒªãƒ—ãƒˆ**ï¼ˆ`src/optuna_search.py`ï¼‰ã‚’ç”¨æ„ã—ã¦ã„ã¾ã™ã€‚  
`GA-RMSE` ã¨ `|Î”TrP|` ã® **ãƒ‘ãƒ¬ãƒ¼ãƒˆãƒ•ãƒ­ãƒ³ãƒˆ**æ¢ç´¢ã«æœ‰åŠ¹ã§ã™ã€‚

**Basic run**

```bash
python -m src.optuna_search --trials 200 --epochs 500
```

Key options (subset) / ä¸»è¦ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆæŠœç²‹ï¼‰:
- `--lr` (fixed) or `--lr_min --lr_max [--lr_log]`
- `--w_plaq`, `--w_wil{11,12,22,13,23}`, `--w_cr`, regularizers, and Huber deltas can be fixed or ranged via `*_min/*_max`.
- `--search_use_huber True False` to explore {True, False}.

Artifacts (single run) / ç”Ÿæˆç‰©ï¼ˆå˜ç™ºå®Ÿè¡Œæ™‚ï¼‰:
- `all_trials.csv`, `pareto_trials.csv`
- `pareto_scatter.png`
- `topk_by_ga_rmse.json`, `topk_by_abs_dTrP.json`

### two-phase search | 2 æ®µéšæ¢ç´¢

**English**  
`run_optuna.bash` provides a ready-to-run wrapper around `src/optuna_search.py`.  
It performs a **two-phase search (Wide â†’ Boost)** over hyperparameters on a 4Ã—4 SU(2) lattice.  
Artifacts (CSV/JSON/plots/decision) are written under `runs/optuna/<timestamp>__latticebench-once__.../`.

**æ—¥æœ¬èª**  
`run_optuna.bash` ã¯ `src/optuna_search.py` ã‚’å‘¼ã³å‡ºã™å®Ÿè¡Œç”¨ãƒ©ãƒƒãƒ‘ãƒ¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ã™ã€‚  
SU(2) 4Ã—4 æ ¼å­ã«å¯¾ã—ã¦ **2 æ®µéšæ¢ç´¢ï¼ˆWide â†’ Boostï¼‰** ã‚’è¡Œã„ã€æˆæœç‰©ï¼ˆCSV/JSON/å›³ãƒ»åˆ¤å®šçµæœï¼‰ã‚’  
`runs/optuna/<timestamp>__latticebench-once__.../` ã«å‡ºåŠ›ã—ã¾ã™ã€‚

**Run**

```bash
bash run_optuna.bash
# logs -> runs/optuna/run_*.log
# artifacts -> runs/optuna/2025xxxx-xxxxxx__latticebench-once__tr480__ep300__b1200/
```

**Notes | æ³¨æ„**  
- Default setting: 480 trials (Wide, 300 epochs each) â†’ top 15% boosted to 1200 epochs.
  ï¼ˆæ—¢å®šã§ã¯ Wide 480 è©¦è¡Œãƒ»300ã‚¨ãƒãƒƒã‚¯ â†’ ä¸Šä½15%ã‚’ Boost ã§1200ã‚¨ãƒãƒƒã‚¯ã«å»¶é•·ï¼‰
- Results include:
  - `all_trials_base.csv`, `all_trials_boost.csv`
  - `pareto_scatter_base.png`, `pareto_scatter_boost.png`
  - `topk_by_ga_rmse_*.json`, `topk_by_abs_dTrP_*.json`
  - `decision.json` + `_ACCEPT`/`_REJECT` marker
- Like `run.bash`, the `runs/` directory is .gitignore-ed.
   ï¼ˆ`run.bash` åŒæ§˜ã€`runs/` ã¯ `.gitignore` æ¸ˆã¿ï¼‰

---

## ğŸ§¾ Log Analysis & Plots | ãƒ­ã‚°è§£æã¨å¯è¦–åŒ–

**English**  
- `src/analyze_log.py`: parse one training log â†’ **single-row CSV**  
- `src/audit_runs.py`: aggregate multiple logs â†’ **CSV**  
- `src/plot_runs.py`: render Pareto scatter & histograms

**æ—¥æœ¬èª**  
- `src/analyze_log.py`: å˜ä¸€ãƒ­ã‚°ã‚’è§£æã—ã¦ **1 è¡Œ CSV** å‡ºåŠ›  
- `src/audit_runs.py`: è¤‡æ•°ãƒ­ã‚°ã‚’ **é›†è¨ˆ CSV** ã«å¤‰æ›  
- `src/plot_runs.py`: ãƒ‘ãƒ¬ãƒ¼ãƒˆæ•£å¸ƒå›³ã‚„ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã‚’æç”»

Examples / ä¾‹:

```bash
# 1) Aggregate many logs
python -m src.audit_runs runs/*.log --out runs_all.csv --top 20

# 2) Plot summaries
python -m src.plot_runs runs_all.csv --outdir plots --top 20

# (Optional) Visualize single-run CSV from analyze_log.py
python -m src.plot_runs runs.csv --outdir plots_runs --top 30
```

Artifacts / ç”Ÿæˆç‰©:
- `runs_all.csv` â€” one row per log/run  
- `plots/` â€” scatter/pareto & histograms; top-k CSV by `ga_rmse` and by `|Î” avgTrP|`

---

## ğŸ“Š Results of Hyperparameter Search | ãƒã‚¤ãƒ‘ãƒ©æ¢ç´¢ã®çµæœ

**English**  

The following results are as of September 25, 2025.

We executed `run_optuna.bash` on a 4Ã—4 SU(2) lattice, using a two-phase Optuna search (Base â†’ Boost).  
As shown in the Pareto scatter plots below (Base vs. Boost), the **Gauge-aligned RMSE** improved in the Boost phase.  
However, the **trade-off with the mean plaquette error** remained evident, and the Pareto front did not advance significantly.  

![Base](runs/optuna/20250924-131205__latticebench-once__tr480__ep300__b1200/pareto_scatter_base.png)  

![Boost](runs/optuna/20250924-131205__latticebench-once__tr480__ep300__b1200/pareto_scatter_boost.png)  

From these results, it appears that with the **current loss design and model structure**,  
further substantial improvement of the Pareto front is unlikely.  
Future progress may require **redesigning the loss function** (e.g., dynamic weighting, new regularizers) or **revamping the model architecture** (e.g., explicitly gauge-equivariant networks).

All artifacts are stored under:  
`runs/optuna/20250924-131205__latticebench-once__tr480__ep300__b1200/`

At present, while the **mean plaquette error remains difficult to reduce**,  
the **gauge-aligned RMSE is already sufficiently low**.  
However, these solutions are likely **numerical optima specialized to the small 4Ã—4 lattice**,  
and there is **no guarantee that they extrapolate to larger lattices or the continuum limit**.  
Further verification will require the kinds of analyses described in  
**"Model Analysis & Interpretability"**.

---

**æ—¥æœ¬èª**  

ä»¥ä¸‹ã¯ 2025/9/25 æ™‚ç‚¹ã§ã®æ¤œè¨çµæœã§ã™ã€‚

`run_optuna.bash` ã‚’å®Ÿè¡Œã—ã€SU(2) 4Ã—4 æ ¼å­ã§ Optuna ã«ã‚ˆã‚‹ 2 æ®µéšæ¢ç´¢ï¼ˆBase â†’ Boostï¼‰ã‚’è¡Œã„ã¾ã—ãŸã€‚  
ä¸‹å›³ã®ãƒ‘ãƒ¬ãƒ¼ãƒˆæ•£å¸ƒå›³ï¼ˆBase ã¨ Boostï¼‰ã«ç¤ºã™ã‚ˆã†ã«ã€**Gauge-aligned RMSE ã®æ”¹å–„**ã¯ Boost æ®µéšã§ç¢ºèªã§ãã¾ã—ãŸãŒã€  
ä¸€æ–¹ã§ **å¹³å‡ãƒ—ãƒ©ãƒ¼ã‚¯ã‚¨ãƒƒãƒˆèª¤å·®ã¨ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•**ã¯ä¾ç„¶ã¨ã—ã¦æ®‹ã‚Šã€ãƒ‘ãƒ¬ãƒ¼ãƒˆãƒ•ãƒ­ãƒ³ãƒˆã®å¤§ããªå‰é€²ã¯è¦‹ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚  

![Base](runs/optuna/20250924-131205__latticebench-once__tr480__ep300__b1200/pareto_scatter_base.png)  

![Boost](runs/optuna/20250924-131205__latticebench-once__tr480__ep300__b1200/pareto_scatter_boost.png)  

ã“ã®çµæœã‹ã‚‰ã€**ç¾çŠ¶ã® loss è¨­è¨ˆã‚„ãƒ¢ãƒ‡ãƒ«æ§‹é€ ã§ã¯ãƒ‘ãƒ¬ãƒ¼ãƒˆãƒ•ãƒ­ãƒ³ãƒˆã®å¤§å¹…ãªæ”¹å–„ã¯è¦‹è¾¼ã‚ãªã„**ã“ã¨ãŒåˆ†ã‹ã‚Šã¾ã—ãŸã€‚  
ä»Šå¾Œã®æ”¹å–„ã«ã¯ã€**loss ã®å†è¨­è¨ˆ**ï¼ˆå‹•çš„é‡ã¿ä»˜ã‘ã€æ–°ã—ã„æ­£å‰‡åŒ–é …ãªã©ï¼‰ã‚„ã€**ãƒ¢ãƒ‡ãƒ«æ§‹é€ ã®åˆ·æ–°**ï¼ˆã‚²ãƒ¼ã‚¸å¯¾ç§°æ€§ã‚’çµ„ã¿è¾¼ã‚“ã ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãªã©ï¼‰ãŒå¿…è¦ã«ãªã‚‹ã¨è€ƒãˆã‚‰ã‚Œã¾ã™ã€‚  

æˆæœç‰©ã¯ä»¥ä¸‹ã«æ ¼ç´ã•ã‚Œã¦ã„ã¾ã™ï¼š  
`runs/optuna/20250924-131205__latticebench-once__tr480__ep300__b1200/`

ç¾æ™‚ç‚¹ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€**å¹³å‡ãƒ—ãƒ©ãƒ¼ã‚¯ã‚¨ãƒƒãƒˆèª¤å·®ãŒä¸‹ãŒã‚Šãã‚‰ãªã„**ä¸€æ–¹ã€ **Gauge-aligned RMSE ã¯ååˆ†ã«ä½ããªã£ã¦ã„ã¾ã™**ã€‚
ãŸã ã—ã“ã®ç‚¹ã«ã¤ã„ã¦ã‚‚ã€å¾—ã‚‰ã‚ŒãŸè§£ã¯ **ã€Œå°ã•ãª 4Ã—4 æ ¼å­ã€ã«ç‰¹åŒ–ã—ãŸæ•°å€¤çš„æœ€é©è§£**ã§ã‚ã‚‹å¯èƒ½æ€§ãŒé«˜ãã€  
ãã®ã¾ã¾ **ã‚ˆã‚Šå¤§ããªæ ¼å­ã‚„é€£ç¶šæ¥µé™ã«å¤–æŒ¿ã§ãã‚‹ä¿è¨¼ã¯ã‚ã‚Šã¾ã›ã‚“**ã€‚  
è©³ç´°ãªç¢ºèªã®ãŸã‚ã«ã¯ã€Œãƒ¢ãƒ‡ãƒ«è§£æã¨å¯è§£é‡ˆæ€§ã€ã§ç¤ºã—ãŸã‚ˆã†ãªåˆ†æãŒå¿…è¦ã«ãªã‚Šã¾ã™ã€‚

---

## ğŸ’¬ Motivation | å‹•æ©Ÿ

**English**  
- Traditional lattice QCD research has mainly emphasized precise numerical verification of known theories, with large HPC-oriented codes.
- To the best of my knowledge, there is currently no lightweight, open platform dedicated to exploring new discrete structures or NN-based inductive biases.
- LatticeBench aims to serve as a seed platform for such exploratory work.

**æ—¥æœ¬èª**  
- å¾“æ¥ã®æ ¼å­ QCD ç ”ç©¶ã¯ã€ä¸»ã« æ—¢çŸ¥ã®ç†è«–ã®æ•°å€¤çš„ãªç²¾å¯†æ¤œè¨¼ ã«ç„¦ç‚¹ãŒç½®ã‹ã‚Œã€å¤§è¦æ¨¡ãª HPC ã‚³ãƒ¼ãƒ‰ãŒç”¨ã„ã‚‰ã‚Œã¦ãã¾ã—ãŸã€‚
- ç§ã®æŠŠæ¡ã™ã‚‹é™ã‚Šã§ã¯ã€æ–°ã—ã„é›¢æ•£æ§‹é€ ã‚„ NN ãƒ™ãƒ¼ã‚¹ã®å¸°ç´çš„ãƒã‚¤ã‚¢ã‚¹ã‚’è©¦ã™è»½é‡ã‚ªãƒ¼ãƒ—ãƒ³åŸºç›¤ ã¯å­˜åœ¨ã—ã¦ã„ã¾ã›ã‚“ã€‚
- LatticeBench ã¯ãã®ã‚ˆã†ãªæ¢ç´¢ã®ãŸã‚ã® ã‚·ãƒ¼ãƒ‰ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ  ã‚’ç›®æŒ‡ã—ã¦ã„ã¾ã™ã€‚

---

## ğŸ“ Note | æ³¨è¨˜

**English**  
I myself studied lattice QCD about 20 years ago at university.
This code was originally generated while chatting with ChatGPT about old memories,
but I decided to publish it here just in case it may be useful.

**æ—¥æœ¬èª**  
ç§ã¯ 20 å¹´ã»ã©å‰ã«å¤§å­¦ã§æ ¼å­ QCD ã‚’ç ”ç©¶ã—ã¦ã„ã¾ã—ãŸã€‚
ã“ã®ã‚³ãƒ¼ãƒ‰ã¯ã€ChatGPT ã¨ã®ãŸã‚ã„ãªã„æ€ã„å‡ºè©±ã®ä¸­ã§ç”Ÿæˆã•ã‚ŒãŸã‚‚ã®ã§ã™ãŒã€
å¿µã®ãŸã‚å…¬é–‹ã—ã¦ãŠãã¾ã™ã€‚

---

## ğŸ“œ License | ãƒ©ã‚¤ã‚»ãƒ³ã‚¹
MIT
