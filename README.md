# LatticeBench â€” A Discrete Gauge Theory Playground (PoC)

**English / æ—¥æœ¬èª**

---

## ğŸ§© Overview | æ¦‚è¦

**English**  
**LatticeBench** is a **minimal proof-of-concept (PoC)** platform for experimenting with
**discrete space-time structures** using a
**Physics-Informed Neural Network (PINN) on a 2D SU(2) lattice**.

In conventional lattice QCD, exploring new physical structures requires
heavy modification of large-scale HPC frameworks (e.g., QUDA, Chroma, openQCD)
and massive computational resources. This makes **testing new theoretical structures
or principles very difficult** for individuals.

LatticeBench aims to be a **lightweight numerical playground** where one can
**embed physical constraints (unitarity, gauge symmetry) into the network structure**,
and **learn to reproduce gauge-invariant observables (plaquette, Wilson loops, etc.)**.

Unlike typical PINNs (which approximate **continuous PDEs**),
LatticeBench attempts to explore **discrete space-time structures themselves** â€”
a direction rarely explored so far.

This is **not a validated physical model**, but intended as an **idea prototype** for those
interested in **structure-informed neural approaches to lattice gauge theory and beyond**.

---

**æ—¥æœ¬èª**  
**LatticeBench** ã¯ã€**æ™‚ç©ºãŒæœ¬è³ªçš„ã«é›¢æ•£çš„ã‹ã‚‚ã—ã‚Œãªã„**ã¨ã„ã†ä»®å®šã®ã‚‚ã¨ã«ã€
**2æ¬¡å…ƒ SU(2) æ ¼å­ä¸Šã§ã®ç‰©ç†ã‚¤ãƒ³ãƒ•ã‚©ãƒ¼ãƒ ãƒ‰ NNï¼ˆPINNï¼‰** ã‚’æ§‹ç¯‰ã™ã‚‹
**æœ€å°é™ã®æ¦‚å¿µå®Ÿè¨¼ï¼ˆPoCï¼‰** å®Ÿè£…ã§ã™ã€‚

å¾“æ¥ã®æ ¼å­ QCD ã§ã¯ã€æ–°ã—ã„ç‰©ç†æ§‹é€ ã‚’æ¤œè¨¼ã™ã‚‹ã«ã¯
QUDAãƒ»Chromaãƒ»openQCD ãªã©ã® **å¤§è¦æ¨¡ HPC ã‚³ãƒ¼ãƒ‰** ã‚’æ”¹é€ ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã€
è†¨å¤§ãªè¨ˆç®—è³‡æºã¨å°‚é–€çŸ¥è­˜ã‚’è¦ã™ã‚‹ãŸã‚ã€**å€‹äººãŒç†è«–æ§‹é€ ã‚’æŸ”è»Ÿã«è©¦ã™ã“ã¨ã¯æ¥µã‚ã¦å›°é›£**ã§ã—ãŸã€‚

LatticeBench ã¯ã€**ç‰©ç†çš„åˆ¶ç´„ï¼ˆãƒ¦ãƒ‹ã‚¿ãƒªãƒ†ã‚£ãƒ»ã‚²ãƒ¼ã‚¸å¯¾ç§°æ€§ï¼‰ã‚’ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹é€ ã«çµ„ã¿è¾¼ã¿**ã€
**ãƒ—ãƒ©ãƒ¼ã‚¯ã‚¨ãƒƒãƒˆã‚„ Wilson ãƒ«ãƒ¼ãƒ—ã¨ã„ã£ãŸã‚²ãƒ¼ã‚¸ä¸å¤‰é‡** ã‚’å†ç¾ã™ã‚‹ã‚ˆã†ã«å­¦ç¿’ã•ã›ã‚‹ã“ã¨ã§ã€
**æ–°ã—ã„ç†è«–æ§‹é€ ã‚’å°è¦æ¨¡ã«æ•°å€¤æ¤œè¨¼ã§ãã‚‹å®Ÿé¨“çš„ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ** ã‚’ç›®æŒ‡ã—ã¾ã™ã€‚

ã“ã‚Œã¯ã€**é€£ç¶šå ´æ–¹ç¨‹å¼ã®è¿‘ä¼¼å™¨ã¨ã—ã¦ä½¿ã‚ã‚Œã¦ããŸ PINN ã‚’**
**ã‚ãˆã¦é›¢æ•£çš„ãªæ™‚ç©ºæ§‹é€ ãã®ã‚‚ã®ã®æ¢ç´¢ã«å¿œç”¨ã™ã‚‹** ã¨ã„ã†ã€
ã“ã‚Œã¾ã§ã«ã»ã¨ã‚“ã©ä¾‹ã®ãªã„è©¦ã¿ã§ã™ã€‚

**ç‰©ç†çš„å¦¥å½“æ€§ã¯æœªæ¤œè¨¼** ã§ã‚ã‚Šã€
**æ§‹é€ åˆ¶ç´„ä»˜ã NN ã‚’æ ¼å­ã‚²ãƒ¼ã‚¸ç†è«–ã«å¿œç”¨ã™ã‚‹ç€æƒ³ã‚’æä¾›ã™ã‚‹** ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ã„ã¾ã™ã€‚

---

## ğŸš§ Limitations & Future Work | åˆ¶ç´„ã¨ä»Šå¾Œã®å±•æœ›

- **Scope (PoC):** This repository favors **experimentability over validity**. No claim of physical correctness.
- **Small lattice / SU(2):** All experiments use a **4Ã—4 periodic lattice** with the **SU(2)** gauge group.
- **Model reuse & inference-only workflows:** *Not assumed* in the current PoC.  
  However, as lattice **dimensionality/complexity increases** (e.g., SU(3) or 3D/4D), saving and **reusing trained models** (for inference or as **warm starts / fine-tuning** bases) may become meaningful.

â€”

- **ã‚¹ã‚³ãƒ¼ãƒ—ï¼ˆPoCï¼‰:** æœ¬ãƒªãƒã‚¸ãƒˆãƒªã¯ **ç‰©ç†çš„ãªå³å¯†æ€§ã‚ˆã‚Šå®Ÿé¨“å®¹æ˜“æ€§ã‚’é‡è¦–** ã—ã¦ã„ã¾ã™ã€‚
- **å°è¦æ¨¡æ ¼å­ / SU(2):** å®Ÿé¨“ã¯ **4Ã—4 å‘¨æœŸæ ¼å­**ãƒ»**SU(2)** ã‚’å‰æã¨ã—ã¦ã„ã¾ã™ã€‚
- **ãƒ¢ãƒ‡ãƒ«å†åˆ©ç”¨ãƒ»æ¨è«–å°‚ç”¨:** ç¾æ®µéšã§ã¯ **å‰æã¨ã—ã¦ã„ã¾ã›ã‚“**ã€‚  
  ãŸã ã—ã€**æ¬¡å…ƒ/è¤‡é›‘ã•ã®æ‹¡å¤§**ï¼ˆä¾‹ï¼šSU(3) ã‚„ 3D/4Dï¼‰ã§ã¯ã€**å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ãƒ»å†åˆ©ç”¨**ï¼ˆæ¨è«–ã‚„ **ã‚¦ã‚©ãƒ¼ãƒ ã‚¹ã‚¿ãƒ¼ãƒˆ / å¾®èª¿æ•´**ï¼‰ã®æœ‰åŠ¹æ€§ãŒé«˜ã¾ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

---

## âš™ï¸ Features | ç‰¹å¾´
- SU(2) gauge group on **4Ã—4 periodic lattice**
- **Structure-aware** approach:
  - SU(2) exponential map for strict unitarity
  - *(Planned)* Gauge factorization: `U_{x,Î¼} = g_x Â· V_Î¼ Â· g_{x+Î¼}â€ `
- **Loss components:**
  - Plaquette traces (complex MSE)
  - Unitarity penalty
  - Smoothness penalty
- **Evaluation metrics:**
  - Mean plaquette trace
  - Wilson loop (1Ã—1, 1Ã—2, 2Ã—2)
  - Creutz ratio Ï‡(2,2)
  - Gauge-aligned link RMSE

---

## ğŸ“ˆ Evaluation | è©•ä¾¡æŒ‡æ¨™
- `|Î”Tr P|`: å¹³å‡ãƒ—ãƒ©ãƒ¼ã‚¯ã‚¨ãƒƒãƒˆèª¤å·®
- `Wilson(Rx,Ry)`: æ±åŒ–ç¢ºèªï¼ˆæœªä½¿ç”¨ã‚µã‚¤ã‚ºå«ã‚€ï¼‰
- `Creutz Ï‡(2,2)`: é¢å¼µåŠ›æ¨å®š
- `Gauge-aligned RMSE`: ã‚²ãƒ¼ã‚¸å¤‰æ›è‡ªç”±åº¦ã‚’é™¤ã„ãŸãƒªãƒ³ã‚¯ RMSE

---

## ğŸ“¦ Requirements | å¿…è¦ç’°å¢ƒ

Minimal requirements (CPU execution):

```
numpy
torch
pandas
matplotlib
```

> Note: `requirements.txt` deliberately excludes GPUâ€‘specific builds of PyTorch.

---

## âš¡ GPU / CUDA (optional but recommended) | GPU / CUDAï¼ˆæ¨å¥¨ï¼‰

Install PyTorch with CUDA support **before** installing `requirements.txt`.
Check the official PyTorch site for the correct build for your system.

### Linux / Windows (CUDA 12.1 example)

```bash
pip install --index-url https://download.pytorch.org/whl/cu121 torch torchvision
pip install -r requirements.txt
```

### CPU only

```bash
pip install torch torchvision   # CPU build
pip install -r requirements.txt
```

### Apple Silicon (MPS)

```bash
pip install torch torchvision   # MPS build
pip install -r requirements.txt
```

### Quick check

```bash
python - <<'PY'
import torch
print("cuda? ", torch.cuda.is_available())
if torch.cuda.is_available():
    print("gpu  : ", torch.cuda.get_device_name(0))
print("mps?  ", torch.backends.mps.is_available())
PY
```

---

## ğŸš€ Usage | ä½¿ã„æ–¹

Run a single training:

```bash
python -m src.train \
  --epochs 700 --print_every 100 \
  --lr 5e-3 --seed 777 \
  --use_huber --huber_delta_wil 0.003 --huber_delta_cr 0.008 \
  --w_wil11 0.10 --w_wil12 0.56 --w_wil22 0.28 --w_wil13 0.20 --w_wil23 0.16 \
  --w_cr 0.55 \
  --w_plaq 0.04 \
  --w_unitary 0.06 \
  --w_phi_smooth 0.04 --w_theta_smooth 0.02 --w_phi_l2 0.002
```

Outputs are written to `runs/*.log`.

---

## ğŸ§ª Batch Sweep (run.bash) | ä¸€æ‹¬ã‚¹ã‚¤ãƒ¼ãƒ—

**English**  
`run.bash` launches a grid search over several loss weights, learning rates, Huber on/off, and seeds.
Each run writes a human-readable log to `runs/*.log`. It can take many hours on CPU.

**æ—¥æœ¬èª**  
`run.bash` ã¯è¤‡æ•°ã®æå¤±é‡ã¿ãƒ»å­¦ç¿’ç‡ãƒ»Huber æœ‰ç„¡ãƒ»ã‚·ãƒ¼ãƒ‰ã®ã‚°ãƒªãƒƒãƒ‰æ¢ç´¢ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
å„å®Ÿè¡Œã®ãƒ­ã‚°ã¯ `runs/*.log` ã«ä¿å­˜ã•ã‚Œã¾ã™ã€‚CPU ã§ã¯é•·æ™‚é–“ï¼ˆæ•°æ™‚é–“ã€œï¼‰ã‹ã‹ã‚Šã¾ã™ã€‚

### Run

```bash
bash run.bash
# logs -> runs/A_*.log, runs/B_*.log
```

**Notes | æ³¨æ„**  
- Edit arrays in the header of `run.bash` to adjust the sweep.  
  ï¼ˆã‚¹ã‚¯ãƒªãƒ—ãƒˆå…ˆé ­ã®é…åˆ—ã‚’ç·¨é›†ã—ã¦æ¢ç´¢ç¯„å›²ã‚’èª¿æ•´ã§ãã¾ã™ï¼‰
- Logs and artifacts are ignored by git via `.gitignore` (`runs/`)ã€‚  
  ï¼ˆ`runs/` ã¯ `.gitignore` æ¸ˆã¿ï¼‰

---

## ğŸ§¾ Single-Log Analysis (analyze_log.py) | å˜ç™ºãƒ­ã‚°è§£æ

**English**  
Parses one training log and writes a single CSV row with metrics and recovered args.

**æ—¥æœ¬èª**  
å˜ä¸€ã®å­¦ç¿’ãƒ­ã‚°ã‚’è§£æã—ã€æŒ‡æ¨™ã¨æ¨å®šå¼•æ•°ã‚’ 1 è¡Œã® CSV ã¨ã—ã¦å‡ºåŠ›ã—ã¾ã™ã€‚

```bash
python -m src.analyze_log runs/A_lr5e-3_w12_0.44_cr0.55_pl0.04_s42.log --out runs.csv
```

**Output columns (subset) | ä¸»ãªåˆ—**  
- `ga_rmse` â€” Gauge-aligned link RMSE
- `avgTrP_pred`, `avgTrP_true`, `avgTrP_absdiff`
- `W1x1_*`, `W1x2_*`, `W2x2_*`, `chi_2x2_*`
- recovered args (e.g., `arg_lr`, `arg_seed`, weightsâ€¦)

> Tip: Keep the **â€œCMD:â€ line** in logs (printed by `train.py`) so arguments are recovered exactly.  
> ï¼ˆãƒ­ã‚°ã« **CMD è¡Œ** ãŒã‚ã‚‹ã¨å¼•æ•°ã‚’å®Œå…¨å¾©å…ƒã§ãã¾ã™ï¼‰

---

## ğŸ“Š Aggregation & Plots | é›†è¨ˆã¨å¯è¦–åŒ–

**English**  
Aggregate multiple logs into one CSV, then plot top runs.

**æ—¥æœ¬èª**  
è¤‡æ•°ãƒ­ã‚°ã‚’ 1 ã¤ã® CSV ã«é›†è¨ˆã—ã€ä¸Šä½çµæœã‚’å¯è¦–åŒ–ã—ã¾ã™ã€‚

```bash
# 1) Aggregate
python -m src.audit_runs runs/*.log --out runs_all.csv --top 20

# 2) Plot summaries
python -m src.plot_runs runs_all.csv --outdir plots --top 20

# (Optional) Visualize single-run CSV from analyze_log.py
python -m src.plot_runs runs.csv --outdir plots_runs --top 30
```

**Artifacts | ç”Ÿæˆç‰©**  
- `runs_all.csv` â€” one row per log/runï¼ˆãƒ­ã‚° 1 å€‹ = 1 è¡Œï¼‰
- `plots/` â€” scatter/pareto & histograms; top-k CSV by `ga_rmse` and by `|Î” avgTrP|`
  ï¼ˆæ•£å¸ƒå›³ãƒ»ãƒ‘ãƒ¬ãƒ¼ãƒˆãƒ»ãƒ’ã‚¹ãƒˆã€`ga_rmse` ã¨ `|Î” avgTrP|` ã®ä¸Šä½ã‚’ CSV ã§å‡ºåŠ›ï¼‰

---

## ğŸ“ˆ Results & Discussion | çµæœã¨è€ƒå¯Ÿ

This section summarizes the batch experiment driven by **`run.bash`**, the search space, and the **best settings** we observed under two evaluation priorities.
ï¼ˆã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ **`run.bash`** ã«ã‚ˆã‚‹ä¸€æ‹¬å®Ÿé¨“ã€ãã®æ¢ç´¢ç¯„å›²ã€ãã—ã¦ **è©•ä¾¡æŒ‡æ¨™ã”ã¨ã®æœ€è‰¯è¨­å®š** ã‚’ã¾ã¨ã‚ã¾ã™ã€‚ï¼‰

### ğŸ”§ What was run (run.bash) | å®Ÿé¨“ã®å†…å®¹

We launched a grid over selected loss weights, learning rates, and loss types (MSE vs Huber). Each setting ran for **900 epochs** and wrote a humanâ€‘readable log under `runs/*.log`.
ï¼ˆæå¤±é‡ã¿ãƒ»å­¦ç¿’ç‡ãƒ»æå¤±ç¨®åˆ¥ï¼ˆMSE / Huberï¼‰ã«å¯¾ã—ã¦ã‚°ãƒªãƒƒãƒ‰æ¢ç´¢ã‚’å®Ÿè¡Œã€‚å„è¨­å®šã¯ **900 ã‚¨ãƒãƒƒã‚¯** å­¦ç¿’ã—ã€`runs/*.log` ã«ãƒ­ã‚°ã‚’ä¿å­˜ã€‚ï¼‰

**Fixed base (common to all runs) | å…±é€šè¨­å®š**
```bash
python -m src.train --epochs 900 --print_every 50 \
  --w_unitary 0.06 \
  --w_phi_smooth 0.04 --w_theta_smooth 0.02 --w_phi_l2 0.002 \
  --w_wil11 0.10 --w_wil22 0.28 --w_wil13 0.22 --w_wil23 0.18
```
- These weights enforce **unitarity** and regularization (smoothness/L2), and include Wilson 1Ã—1/2Ã—2/1Ã—3/2Ã—3 components at fixed strengths.  
  ï¼ˆ**ãƒ¦ãƒ‹ã‚¿ãƒªãƒ†ã‚£**ã¨æ­£å‰‡åŒ–ï¼ˆã‚¹ãƒ ãƒ¼ã‚¹/L2ï¼‰ã‚’èª²ã—ã€Wilson 1Ã—1/2Ã—2/1Ã—3/2Ã—3 ã‚’å›ºå®šé‡ã¿ã§å«ã‚ã¾ã™ã€‚ï¼‰

**Swept hyperparameters | æ¢ç´¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**
- **Plaquette weight `w_plaq`**: `0.02`, `0.04`, `0.06`
- **Wilson (1Ã—2) weight `w_wil12`**: `0.32`, `0.44`, `0.56`
- **Creutz ratio weight `w_cr`**: `0.45`, `0.55`, `0.65`
- **Learning rate `lr`**: `5e-3`, `4e-3`
- **Seed**: `42` (fixed)
- **Loss type**: **A=MSE**, **B=Huber** with `--huber_delta_wil 0.003 --huber_delta_cr 0.008`

> Total grid size = 3 (plaq) Ã— 3 (w12) Ã— 3 (cr) Ã— 2 (lr) Ã— 1 (seed) Ã— 2 (loss) = **108 runs**.

**Launch script (excerpt) | å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆæŠœç²‹ï¼‰**
```bash
## Pattern A: MSE
${BASE} --lr ${lr} --w_wil12 ${w12} --w_cr ${wcr} --w_plaq ${wpl} --seed ${seed} \
  | tee "runs/A_lr${lr}_w12${w12}_cr${wcr}_pl${wpl}_s${seed}.log"

## Pattern B: Huber (delta small, sharper)
${BASE} --lr ${lr} --w_wil12 ${w12} --w_cr ${wcr} --w_plaq ${wpl} --seed ${seed} \
  --use_huber --huber_delta_wil 0.003 --huber_delta_cr 0.008 \
  | tee "runs/B_lr${lr}_w12${w12}_cr${wcr}_pl${wpl}_s${seed}.log"
```

### ğŸ§ª Metrics | è©•ä¾¡æŒ‡æ¨™
- **Gaugeâ€‘aligned RMSE** (lower better): linkâ€‘wise error after gauge alignment.
- **|Î” avgTrP|** (lower better): absolute error of the **mean plaquette trace**.
- We treat them as **two objectives**; a single setting rarely minimizes both.  
  ï¼ˆä¸¡æŒ‡æ¨™ã¯ã—ã°ã—ã°ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã«ãªã‚Šã¾ã™ã€‚ï¼‰

### ğŸ¥‡ Best Settings | æœ€è‰¯è¨­å®šï¼ˆæŒ‡æ¨™åˆ¥ï¼‰

**A. Minimize Gaugeâ€‘aligned RMSEï¼ˆGAâ€‘RMSE æœ€å°ï¼‰**  
- **Observed**: `ga_rmse â‰ˆ 0.356` (global best in this sweep)
- **Tradeâ€‘off**: `|Î” avgTrP| â‰ˆ 0.545` (plaquette mismatch remains)
- **Tendencies**: **Huber** helps RMSE stability; smaller `w_plaq` avoids overâ€‘fitting avgTrP.
- **Command to reproduce:**
```bash
python -m src.train \
  --epochs 900 --print_every 50 \
  --lr 5e-3 --seed 42 \
  --use_huber --huber_delta_wil 0.003 --huber_delta_cr 0.008 \
  --w_wil11 0.10 --w_wil12 0.32 --w_wil22 0.28 --w_wil13 0.22 --w_wil23 0.18 \
  --w_cr 0.45 \
  --w_plaq 0.02 \
  --w_unitary 0.06 \
  --w_phi_smooth 0.04 --w_theta_smooth 0.02 --w_phi_l2 0.002
```

**B. Minimize |Î” avgTrP|ï¼ˆå¹³å‡ãƒ—ãƒ©ãƒ¼ã‚¯ã‚¨ãƒƒãƒˆèª¤å·® æœ€å°ï¼‰**  
- **Observed**: `|Î” avgTrP| â‰ˆ 0.0015` (nearâ€‘perfect match)
- **Tradeâ€‘off**: `ga_rmse â‰ˆ 0.412` (slightly worse alignment)
- **Tendencies**: **MSE** + **higher `w_plaq`** helps pin the mean plaquette tightly.
- **Command to reproduce:**
```bash
python -m src.train \
  --epochs 900 --print_every 50 \
  --lr 5e-3 --seed 42 \
  --w_wil11 0.10 --w_wil12 0.32 --w_wil22 0.28 --w_wil13 0.22 --w_wil23 0.18 \
  --w_cr 0.45 \
  --w_plaq 0.06 \
  --w_unitary 0.06 \
  --w_phi_smooth 0.04 --w_theta_smooth 0.02 --w_phi_l2 0.002
```

> **Pareto view**: These two lie near the empirical **Pareto front** (improving one metric worsens the other).
> ï¼ˆçµŒé¨“çš„ãƒ‘ãƒ¬ãƒ¼ãƒˆå‰ç·šä¸Šã«è¿‘ã„ä½ç½®ã«ã‚ã‚Šã¾ã™ã€‚ï¼‰

### ğŸ’¡ Practical Guidance | å®Ÿå‹™ã®æŒ‡é‡
- **If you care about local link quality (alignment)** â†’ prefer **Huber** + lower `w_plaq`.
- **If you care about global average matching** â†’ prefer **MSE** + higher `w_plaq`.
- Start with `lr=5e-3`; try `4e-3` if loss plateaus or oscillates.
- Keep `w_wil12` and `w_cr` moderateâ€‘toâ€‘high (`0.44â€“0.56`, `0.55â€“0.65`) when you want Wilson(1Ã—2)/Creutz to generalize beyond plaquette.

### âš ï¸ Caveats | æ³¨æ„ç‚¹
- This is a **PoC**, not a validated physical model.
- Metrics are computed on a fixed small lattice (4Ã—4, SU(2)); scaling behavior may differ.
- Huber/MSE preference can invert if you substantially change the loss mix or schedule.

---

## ğŸ’¬ Motivation | å‹•æ©Ÿ

**English**  
- Lattice QCD focuses on precise verification rather than theory discovery; codes are large and HPCâ€‘oriented.
- There is **no lightweight, open platform** for trying new discrete structures or NNâ€‘based inductive biases.
- LatticeBench aims to serve as a **seed platform** for such explorations.

**æ—¥æœ¬èª**  
- æ ¼å­ QCD ã¯ã€Œç†è«–ç™ºè¦‹ã€ã‚ˆã‚Šã€Œç²¾å¯†æ¤œè¨¼ã€ã«é‡ç‚¹ãŒç½®ã‹ã‚Œã€ã‚³ãƒ¼ãƒ‰ã¯å¤§è¦æ¨¡ã§ HPC ä¾å­˜ã§ã™ã€‚
- **æ–°ã—ã„é›¢æ•£æ§‹é€ ã‚„ NN ãƒ™ãƒ¼ã‚¹ã®å¸°ç´çš„ãƒã‚¤ã‚¢ã‚¹ã‚’è©¦ã™è»½é‡ã‚ªãƒ¼ãƒ—ãƒ³åŸºç›¤** ã¯å­˜åœ¨ã—ã¾ã›ã‚“ã€‚
- LatticeBench ã¯ãã®ã‚ˆã†ãªæ¢ç´¢ã®ãŸã‚ã® **ã‚·ãƒ¼ãƒ‰ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ** ã‚’ç›®æŒ‡ã—ã¦ã„ã¾ã™ã€‚

---

## ğŸ“ Note | æ³¨è¨˜

**English**  
I myself studied lattice QCD about 20 years ago at university.
This code was originally generated while chatting with ChatGPT about old memories,
but I decided to publish it here just in case it may be useful.

**æ—¥æœ¬èª**  
ç§ã¯ 20 å¹´ã»ã©å‰ã«å¤§å­¦ã§æ ¼å­ QCD ã‚’ç ”ç©¶ã—ã¦ã„ã¾ã—ãŸã€‚
ã“ã®ã‚³ãƒ¼ãƒ‰ã¯ã€ChatGPT ã¨ã®ãŸã‚ã„ãªã„æ€ã„å‡ºè©±ã®ä¸­ã§ç”Ÿæˆã•ã‚ŒãŸã‚‚ã®ã§ã™ãŒã€
å¿µã®ãŸã‚å…¬é–‹ã—ã¦ãŠãã¾ã™ã€‚

---

## ğŸ“œ License | ãƒ©ã‚¤ã‚»ãƒ³ã‚¹
MIT
